{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd6NXqG5RgBc+K3YM7guUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/848498/Web-scraping-assignment-4/blob/main/Web_scraping_assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clpKLs01VnyW"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"http://en.wikipedia.org/wiki/list_of_most-viwed_You_Tube_videos\"\n",
        "responses = requests.get(url)\n",
        "content = responses.content\n",
        "soup = BeautifulSoup(content,\"html.parser\")\n",
        "table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
        "for row in table.find_all(\"tr\")[1:]\n",
        "cells = row.find_all(\"td\")\n",
        "rank = cells[0].text.strip()\n",
        "artist = cells[2].text.strip()\n",
        "upload_date = cells[4].text.strip\n",
        "views = cells[1].text.strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"http://statisticstime.com\"\n",
        "responses = requests.get(url)\n",
        "soup = BeautifulSoup(content,\"html.parser\")\n",
        "economy_link = soup.select_one(\"a[href*='economy']\")[\"href\"]\n",
        "economy_url = url + economy_link\n",
        "economy_responses = requests.get(economy_url)\n",
        "economy_soup = BeautifulSoup(economy_responses.content, \"html.parser \")\n",
        "gdp_table = economy_soup.select_one(\"table#table_id\")\n",
        "data = []\n",
        "rows = gdp_table.select(\"tbody tr\")\n",
        "cells = row.select(\"td\")\n",
        "rank = cells[0].text.strip()\n",
        "state = cells[1].text.strip()\n",
        "gdp_18_19 = cells[2].text.strip()\n",
        "gdp_19_20 = cells[3].text.strip()\n",
        "share_18_19 = cells[4].text.strip()\n",
        "gdp_billion = cells[5].text.strip()\n",
        "data.append({\n",
        "\"Rank\": rank\n",
        "\"State\": state\n",
        "\"GSDP(18_19)\":gdp_18_19\n",
        "\"GSDP(19_20)\":gdp_19_20\n",
        "\"Share(18_19)\":share_18_19\n",
        "\"GDP($ billion)\":gdp_billion\n",
        ")}\n",
        "for item in data:\n",
        "  print(item)\n",
        "\n"
      ],
      "metadata": {
        "id": "HCUNH0bucS3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"http://github.com/\"\n",
        "response =  requests.get(url)\n",
        "soup = BeautifulSoup( response.content, \"html.parser\")\n",
        "explore_menu = soup.find(\"nav\", class_=\"UnderlineNav-body\")\n",
        "trending_link = explore_menu.find(\"a\", href=\"/trending\")\n",
        "trending_url = \"https://github.com\" + trending_link[\"href\"]\n",
        "trending_response = requests.get(trending_url)\n",
        "trending_soup = BeautifulSoup(trending_response.content, \"html.parser\")\n",
        "repositories = trending_soup.find_all(\"article\", class_=\"Box-row\")\n",
        "title = repo.find(\"h1\").text.strip()\n",
        "description = repo.find(\"p\", class_=\"col-9\").text.strip()\n",
        "contributors = repo.find(\"a\", href=lambda href: and \"/contributors\" in href)\n",
        "contributors_count = contributors.text.strip() if contributors else \"0\"\n",
        "language = repo.find(\"span\", itemprop=\"programmingLanguage\")\n",
        "language_used = language.text.strip() if language else \"Not specified\"\n",
        "print(\"Title:\", title)\n",
        "print(\"Description:\", description)\n",
        "print(\"Contributors count:\", contributors_count)\n",
        "print(\"Language used:\", language_used)\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "id": "c0m5GZNJXRaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://billboard.com/\"\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "charts_link = soup.find(\"a\", text=\"charts\")\n",
        "hot100_link = charts_link[\"href\"]\n",
        "hot100_url = url + hot100_link\n",
        "hot100_response = requests.get(hot100_url)\n",
        "hot100_html_content = hot100_response.content\n",
        "hot100_soup = BeautifulSoup(hot100_html_content, \"html.parser\")\n",
        "table = hot100_soup.find(\"table\", class_=\"charts-list\")\n",
        "for row in table.find_all(\"tr\"):\n",
        "  columns = row.find_all(\"td\")\n",
        "  song_name = columns[1].text.strip()\n",
        "  artist_name = columns[2].text.strip()\n",
        "  last_week_rank = columns[3].text.strip()\n",
        "  peak_rank = columns[4].text.strip()\n",
        "  week_on_board = columns[5].text.strip\n"
      ],
      "metadata": {
        "id": "UejibHX_isD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "book_elements = soup.select(\"datablog table tbody tr\")\n",
        "books = []\n",
        "for bool_element in book_elements:\n",
        "  book_name = book_elements.select_one(\"td:nth-child(2)\").text.strip()\n",
        "  author_name = book_elements.select_one(\"td:nth-child(3)\").text.strip()\n",
        "  volumes_sold = book_elements.select_one(\"td:nth-child(4)\").text.strip()\n",
        "  publisher = book_elements.select_one(\"td:nth-child(5)\").text.strip()\n",
        "  genre = book_elements.select_one(\"td:nth-child(6)\").text.strip()\n",
        "  book = {\n",
        "      \"Book Name\": book_name,\n",
        "      \"Author Name\": author_name,\n",
        "      \"Volumes Sold\": volumes_sold,\n",
        "      \"Publisher\": publisher,\n",
        "      \"Genre\": genre\n",
        "  }\n",
        "  books.append(book)\n",
        "  print(book)"
      ],
      "metadata": {
        "id": "keap8OYM2ZFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https:/www.imdb.com/list/1s095964455/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content,\"html.parser\")\n",
        "series_container = soup.find(\"div\", class_=\"lister-list\")\n",
        "series_list = series_container.find_all(\"div\", class_=\"lister-item mode-detail\")\n",
        "for series in series_list\n",
        "name = series.find(\"h3\").find(\"a\").text\n",
        "year_span = series.find(\"span\", class_=\"lister-item-year\").text.strip(\"()\")\n",
        "genre = series.find(\"span\", class_=\"genre\").text.strip()\n",
        "run_time = series.find(\"span\", class_=\"runtime\").text.strip()\n",
        "ratings = series.find(\"div\", class_=\"ipl-rating-star\").find(\"span\", class_=\"ipl-rating-star_rating\")\n",
        "votes = series.find(\"span\", attrs={\"name\": \"nv\"}).text\n",
        "print(\"Name:\", name)\n",
        "print(\"Year Span:\", year_span)\n",
        "print(\"Genre:\", genre)\n",
        "print(\"Run Time:\", run_time)\n",
        "print(\"Ratings:\", ratings)\n",
        "print(\"Votes:\", votes)\n",
        "print()"
      ],
      "metadata": {
        "id": "wERaCaZE8NZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "response = requests.get(\"https://archive.ics.uci.edu/ml/datasets.php\")\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "table = soup.find(\"table\", {\"border\": \"1\"})\n",
        "for row in table.find_all(\"tr\"):\n",
        "  columns = row.find_all(\"td\")\n",
        "  if len (columns) >=7:\n",
        "    dataset_name = columns[0].text.strip()\n",
        "    data_type = columns[1].text.strip()\n",
        "    task = columns[2].text.strip()\n",
        "    attribute_type = columns[3].text.strip()\n",
        "    num_instances = columns[4].text.strip()\n",
        "    num_attributes = columns[5].text.strip()\n",
        "    year = columns[6].text.strip()\n",
        "    print(\"Dataset Name:\", dataset_name)\n",
        "    print(\"Data Type:\", data_type)\n",
        "    print(\"Task:\", task)\n",
        "    print(\"Attribute Type:\", attribute_type)\n",
        "    print(\"Number of Instances:\", num_instances)\n",
        "    print(\"Number of Attributes:\", num_attributes)\n",
        "    print(\"Year:\", year)\n",
        "    print()"
      ],
      "metadata": {
        "id": "5-KjWCkJqFvQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}